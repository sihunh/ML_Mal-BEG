import numpy as np
import pandas as pd
import os
import joblib
import magic
from collections import Counter
import pefile
import re
from concurrent.futures import ThreadPoolExecutor
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from tqdm import tqdm
from sklearn.feature_extraction import DictVectorizer
import yaml

# 설정 파일 로드
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

#동작 방식 
# 모델 학습 -> joblib로 파일 생성 -> Malware_detectmodel에서 모델 로드 -> Upload_page.py에서는 Malware_detect 모델에서 판단한 정보 가져옴

# 학습 방법
# benign 파일, malware 파일 두개로 학습하여 분류 모델 생성 하고 upload_page에서 업로드한 파일 malware인지 benign 인지 판단 
# 모델은 randomforest 사용 하여 학습 
# 벡터라이저는 DictVectorizer 사용 
# 모델, 벡터라이저 저장은 joblib 사용 
# dataset은 DikeDataset을 이용해여 학습 진행

# 기능
# 특징 추출은 정적 분석, 동적 분석 두가지 방법 사용 
# 정적 분석은 파일 메타데이터, 바이트 수준 분석, 문자열 분석, PE 파일 구조 분석 4가지 방법 사용 
# 동적 분석은 아직 미구현 
# 문자열 분석에서 의심스러운 문자열 패턴 검색 추가 필요 


# 특징 추출 함수 
def extract_features(file_path):
    features = {}
    
    # 1. 정적 분석
    features.update(static_analysis(file_path))
    
    # 2. 동적 분석 (선택적)
    # Sandbox 환경 개발? 해봐야하나
    # 주의: 동적 분석은 안전한 환경에서만 수행
    # features.update(dynamic_analysis(file_path))
    
    return features

def static_analysis(file_path):
    features = {}
    
    # a) 파일 메타데이터
    features.update(extract_metadata(file_path))
    
    # b) 바이트 수준 분석
    features.update(byte_level_analysis(file_path))
    
    # c) 문자열 분석
    features.update(string_analysis(file_path))
    
    # d) PE 파일 구조 분석 (Windows 실행 파일의 경우)
    if features['file_type'] == 'PE':
        features.update(pe_analysis(file_path))
    
    return features

def extract_metadata(file_path):
    features = {}
    
    # 파일 크기
    features['file_size'] = os.path.getsize(file_path)
    
    # 생성 날짜, 수정 날짜
    stat = os.stat(file_path)
    features['creation_time'] = stat.st_ctime
    features['modification_time'] = stat.st_mtime
    
    # 파일 형식
    try:
        file_type = magic.from_file(file_path)
        if "PE32" in file_type:
            features['file_type'] = 'PE'
        elif "ELF" in file_type:
            features['file_type'] = 'ELF'
        else:
            features['file_type'] = 'Other'
    except:
        features['file_type'] = 'Unknown'
    
    return features

def byte_level_analysis(file_path):
    features = {}
    
    with open(file_path, 'rb') as f:
        content = f.read()
    
    # 바이트 히스토그램
    byte_hist = Counter(content)
    for i in range(256):
        features[f'byte_{i}'] = byte_hist[i]
    
    # 엔트로피 계산
    features['entropy'] = calculate_entropy(content)
    
    # n-gram 분석 (예: 2-gram)
    ngrams = Counter(zip(content, content[1:]))
    for i, count in enumerate(ngrams.most_common(100)):
        features[f'2gram_{i}'] = count[1]
    
    return features

def string_analysis(file_path):
    features = {}
    
    with open(file_path, 'rb') as f:
        content = f.read()
    
    strings = extract_strings(content)
    
    # 의심스러운 문자열 패턴 검색 이거 몇개 추가 필요 ㅇㅇ
    suspicious_patterns = [
        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
        r'\b(?:cmd\.exe|powershell\.exe|rundll32\.exe)\b',
        r'\b(?:CreateProcess|VirtualAlloc|WriteProcessMemory)\b',
        r'\b(?:RegisterHotKey|SetWindowsHookEx)\b',
        r'\b(?:RegCreateKeyEx|RegSetValueEx)\b'
    ]
    
    for i, pattern in enumerate(suspicious_patterns):
        features[f'suspicious_pattern_{i}'] = len(re.findall(pattern, ' '.join(strings)))
    
    return features

def pe_analysis(file_path):
    features = {}
    
    try:
        pe = pefile.PE(file_path)
        
        # 섹션 정보
        features['num_sections'] = len(pe.sections)
        for i, section in enumerate(pe.sections):
            features[f'section_{i}_name'] = section.Name.decode().rstrip('\x00')
            features[f'section_{i}_size'] = section.SizeOfRawData
            features[f'section_{i}_entropy'] = section.get_entropy()
        
        # 임포트 테이블
        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
            features['num_imports'] = len(pe.DIRECTORY_ENTRY_IMPORT)
            for i, entry in enumerate(pe.DIRECTORY_ENTRY_IMPORT):
                features[f'import_{i}'] = entry.dll.decode()
        
        # 익스포트 테이블
        if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
            features['num_exports'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)
        
        # 리소스 섹션 분석
        if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
            features['has_resources'] = 1
            features['num_resources'] = len(pe.DIRECTORY_ENTRY_RESOURCE.entries)
        else:
            features['has_resources'] = 0
            features['num_resources'] = 0
        
        pe.close()
    except:
        features['pe_parsing_failed'] = 1
    
    return features

def calculate_entropy(data):
    entropy = 0
    for x in range(256):
        p_x = float(data.count(x))/len(data)
        if p_x > 0:
            entropy += - p_x*np.log2(p_x)
    return entropy

def extract_strings(data):
    strings = []
    current_string = ""
    for byte in data:
        if 32 <= byte <= 126:  # printable ASCII characters
            current_string += chr(byte)
        elif len(current_string) > 4:
            strings.append(current_string)
            current_string = ""
        else:
            current_string = ""
    if len(current_string) > 4:
        strings.append(current_string)
    return strings

def dynamic_analysis(file_path):
    # 주의: 이 함수는 안전한 샌드박스 환경에서만 실행
    features = {}
    
    # 여기에 동적 분석 코드를 구현
    # 예를 들어, 가상 머신에서 파일을 실행하고 행동을 모니터링하는 코드를 작성
    # API 호출, 시스템 변경 사항, 네트워크 활동 등을 기록??
    
    return features

# 데이터 준비
def prepare_data(malware_dir, benign_dir):
    def process_file(args):
        file, label = args
        try:
            if os.path.isfile(file):
                feature = extract_features(file)
                return feature, label
        except Exception as e:
            print(f"파일 처리 중 오류 발생 {file}: {str(e)}")
        return None, None

    malware_files = [(os.path.join(malware_dir, f), 1) for f in os.listdir(malware_dir)]
    benign_files = [(os.path.join(benign_dir, f), 0) for f in os.listdir(benign_dir)]
    all_files = malware_files + benign_files

    features = []
    labels = []
    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
        for feature, label in tqdm(executor.map(process_file, all_files), total=len(all_files)):
            if feature is not None:
                features.append(feature)
                labels.append(label)

    vectorizer = DictVectorizer(sparse=False)
    X = vectorizer.fit_transform(features)

    return np.array(X), np.array(labels), vectorizer

# 모델 학습
def train_model(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config['test_size'], random_state=config['random_state'])
    
    clf = RandomForestClassifier(n_estimators=config['n_estimators'], random_state=config['random_state'])
    clf.fit(X_train, y_train)
    
    y_pred = clf.predict(X_test)
    print(classification_report(y_test, y_pred))
    print(confusion_matrix(y_test, y_pred))
    
    return clf

# 메인 실행 부분
if __name__ == "__main__":
    X, y, vectorizer = prepare_data(config['malware_dir'], config['benign_dir'])
    model = train_model(X, y)
    
    joblib.dump(model, config['model_file'])
    joblib.dump(vectorizer, config['vectorizer_file'])
    print("모델과 벡터라이저가 성공적으로 저장되었습니다.")
